# `chop-shop` :: eschew documents and produce content in usable forms


`text-extractor-analyzer` (a.k.a. `chop-shop`) = tool which extracts text from PDFs (like the old Qiqqa extract tool would've done, but *better*!) & analyzes possible applicable heuristics which would improve the exported text, e.g. 
 
- **word separation** for those PDFs which produce a stream of (English) text without the proper spacing, making keyword search very hard,
- **whitespace cleanup** for the near-opposite behaviour we've also observed: some PDFs spitting out slews of consecutive spaces and/or TAB characters as *variable-width* word separators: these lines could simply have been cleaned up to produce single spaces between words, as the old Qiqqa text extract (`doc` directory tree) *cache* has a format which includes position and size value arguments for each extracted "word"; this way we'ld get a more useful *and cleaner* text output stream.
- **ditto**, but for the more usual and *curious & strange* choices for whitespace separators for higher up the Unicode codepoint range: We've observed plenty PDFs which spit out '?' question marks for these, which would obviously have been whitespace characters from a human perspective.
- **recognize content written in non-WS languages (such as Chinese or Japanese)**, which do not mandate word separators & possibly suggest appropriate word separations. There's various libraries for that out there, but remember: here we enter the heuristics/statistics arena at full tilt: it's not about being 100% accurate 100% of the time, but getting near (or even surpass) 99% accurate 99% of the time. ðŸ˜‰
- **recognize content that's gobbledygook**, i.e. *possibly **obfuscated** by way of font codepoint reshuffling*: we've seen several PDFs exhibit this and here we MAY WANT to turn to our next level of text extraction tooling: the OCR engine. 
  Ideally this one could then be cross-checked against the gobbledygook to produce a statistically max-probability codepoint mapping, as that obfuscation mapping would be consistent across the document. When it's just rubbish and there's no discernable relationship between the gobbledygook output and the OCR result, we MAY also want to consider this is perhaps and extract for a chunk of *mathematics*. May/should we treat such chunks as images for our text + context analysis + export action?
* *possibly* **script** extraction / text-postprocessing to improve text extraction output quality, e.g. apply automatic / *suggested?* spell-checking --> this would be an advanced feature as we MAY want to keep both original and *edited="corrected"* text in the output.
    
    > First idea was to markup or otherwise keep both original and edited text in the same file, but when we want to be able to *easily* use *external* professional-quality tools for comparisons & evaluation (such as Scooter Software's Beyond Compare), **the simplest way to get usable results is to output *two* text files**: one "raw" and one "post-processed", so that we can always see whether the "autocorrect" was actually correct or had just fubarred something rare/unknown to the spell-checker/corrector.
    > **This would then more easily blend in with any subsequent human-user vetted editorial edits to the extracted text**: an ability that is currently NOT available, but which I've been desiring for a *long time* as this allows us to Mechanical Turk any 99% OCR result and lift it up into 100% correct (*vetted*) content: not a requirement for all of us, but something I need as this makes straight citing / quoting from the actual content far easier and thus much more usable: I'm personally not invested or interested in the plagiarism scare at some academia; *value* (in my case) is increased when I can directly quote relevant chunks of original reference content so readers don't have to bother with reading / scanning through the references: that's increasing efficiency in a *business research setting*, where verification of references is only important when you don't trust the information collector / writer of the report that you got from me: *efficiency* requires both *trusting the bearer of the news (**me**)* (& thus citing references and having them available on request (*Qiqqa library*!) is beneficial at that secondary level) and *fast perusal*, i.e. NOT having to wade though tens to thousands of extra referenced papers' pages in order to check I'm not pulling the citations out of my arse. Thus *efficiency* in *business reading* actually *benefits* from quoting chunks of text, which in a student/academia setting would be automatically flagged as "plagiarism".
    >  
    ...

