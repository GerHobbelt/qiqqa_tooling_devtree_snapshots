# LLM training video transcript + notes (DeepLearning.AI)

Large Language Models with Semantic Search


- [Introduction](https://learn.deeplearning.ai/large-language-models-semantic-search/lesson/1/introduction)
- [Keyword Search](https://learn.deeplearning.ai/large-language-models-semantic-search/lesson/2/keyword-search)[
- [Embeddings](https://learn.deeplearning.ai/large-language-models-semantic-search/lesson/3/embeddings)[
- [Dense Retrieval](https://learn.deeplearning.ai/large-language-models-semantic-search/lesson/4/dense-retrieval)[
- [ReRank](https://learn.deeplearning.ai/large-language-models-semantic-search/lesson/5/rerank)[
- [Generating Answers](https://learn.deeplearning.ai/large-language-models-semantic-search/lesson/6/generating-answers)[
- [Conclusion](https://learn.deeplearning.ai/large-language-models-semantic-search/lesson/7/conclusion)[
- [Course Feedback](https://learn.deeplearning.ai/large-language-models-semantic-search/course-feedback)[
- [Community](https://learn.deeplearning.ai/large-language-models-semantic-search/community)




## Introduction

Welcome to this short course, Large Language Models with Semantic Search, built in partnership with Cohere. 

In this course, you'll learn how to incorporate large language models, or LLMs, into information search in your own applications. For example, let's say you run a website with a lot of articles, picture Wikipedia for the sake of argument, or a website with a lot of e-commerce products. 

Even before LLMs, it was common to have keyword search to let people maybe search your site. But with LLMs, you can now do much more. First, you can let users ask questions that your system then searches your site or database to answer. Second, the LLM is also making the retrieve results more relevant to the meaning or the semantics of what the user is asking about. 

I'd like to introduce the instructors for this course, Jay Allamar and Luis Serrano. Both Jay and Luis are experienced machine learning engineers as well as educators. 
I've admired for a long time some highly referenced illustrations that Jay had created to explain transformer networks. He's also co-authoring a book, Hands-On Large Language Models. Luis is the author of the book, Grokking Machine Learning, and he also taught with DeepLearning.ai. Math for Machine Learning. At Cohere, Jay and Luis, together with Neil Amir, have also been working on a site called. LLMU, and have a lot of experience teaching developers to use LLMs. So I was thrilled when they agreed to teach semantic search with LLMs. 

Thanks, Andrew. What an incredible honor it is to be teaching this course with you. Your machine learning course introduced me to machine learning eight years ago, and continues to be an inspiration to continue sharing what I learn. As you mentioned, Luis and I work at Cohere, so we get to advise others in the industry on how to use and deploy large language models for various use cases. We are thrilled to be doing this course to give developers the tools they need to build robust LLM powered apps. We're excited to share what we learned from our experience in the field. Thank you, Jay and Luis. Great to have you with us. 

This course consists of the following topics. 
First, it shows you how to use basic keyword search, which is also called lexical search, which powered a lot of search systems before large language models. It consists of finding the documents that has the highest amount of matching words with the query. 
Then you learn how to enhance this type of keyword search with a method called re-rank. As the name suggests, this then ranks the responses by relevance with the query. 
After this, you learn a more advanced method of search, which has vastly improved the results of keyword search, as it tries to use the actual meaning or the actual semantic meaning of the text with which to carry out the search. This method is called dense retrieval. 
This uses a very powerful tool in natural language processing called embeddings, which is a way to associate a vector of numbers with every piece of text. Semantic search consists of finding the closest documents to the query in the space of embeddings. 
Similar to other models, search algorithms need to be properly evaluated. You also learn effective ways to do this. 
Finally, since LLMs can be used to generate answers, you also learn how to plug in the search results into an LLM and have it generate an answer based on them. Dense retrieval with embeddings vastly improves the question answering capabilities of an LLM as it first searches for and retrieves the relevant documents and it creates an answer from this retrieved information. Many people have contributed to this course. We're grateful for the hard work of Meor Amer, Patrick Lewis, Nils Reimer, and Sebastian Hofstätter from Cohere, as well as on the DeepLearning.ai side, Eddie Shyu and Diala Ezzedine. In the first lesson, you will see how search was done before large language models. From there, we will show you how to improve search using LLMs, including tools such as embeddings and re-rank. That sounds great. And with that, let's dive in and go on to the next video.





## Keyword Search


Welcome to Lesson 1. In this lesson, you will learn how to use keyword search to answer questions using a database. Search is crucial to how we navigate the world. This includes search engines, but it also includes search within apps, so when you search Spotify or YouTube or Google Maps for example. Companies and organizations also need to use keyword search or various methods of search to search their internal documents. Keyword search is the method most commonly used to build search systems. Let's look at how we can use a keyword search system and then look at how language models can improve those systems. Now, in this code example, we'll connect to a database and do some keyword search on it. The first cell installs "weaviate=client". You don't need to run this if you're running this from inside the classroom.  But if you want to download this code and run it on your own environment, client, you would want to install "weaviate=client".  The first code cell we need to run loads the API keys we'll need later in the lesson. And then now we can import Weaviate. This will allow us to connect to an online database. We'll talk about this database. Let's talk about what Weaviate is. Weaviate is an open source database. It has keyword search capabilities, but also vector search capabilities that rely on language models. The API key we'll be using here is public, this is part of a public demo, so the actual key is not a secret and you can use it and we encourage you to use it to access this demo database. Now that we've set the configurations for authentication, let's look at this code that connects the client to the actual database. Now this database is a public database and it contains 10 million records. These are records from Wikipedia. Each cell, each record, a row in this database, is a passage, is a paragraph from Wikipedia. These 10 million records are from 10 different languages. So one million of them are in English and the other nine million are in different languages. And we can choose and filter which language we want to query, and we'll see that later in this lab. After we run this line of code, we make sure that the client is ready and connected. And if we get true, then that means that our local Weaviate client is able to connect to the remote Weaviate database. And then now we're able to do a keyword search on this data set. Let's talk a little bit about keyword search before we look at the code. So let's say you have the query, what color is the grass? And you're searching a very small archive that has these five texts, these five sentences. One says tomorrow is Saturday, one says the grass is green, the capital of Canada is Ottawa, the sky is blue, and a whale is a mammal. So this is a simple example of search. A high-level look at how keyword search works is to compare how many words are in common between the query and the documents. So if we compare how many words are in common between the query and the first sentence, they only share the word is. And so that's one word they have in common. And we can see the counts of every sentence in this archive. We can see that the second sentence has the most words in common with the query, and so keyword search might retrieve that as the answer. So now that we're connected to the database, let's build out the function that will query it. Let's call it "keyword_search" and we'll be building this and going back and forth. So the simplest thing we'll need to do here is to say "response = (" and then "client.query.get". Now everything we're doing here, this is Weaviate. So this is all defined by the Weaviate API. And it tells us the kind of data, I think the collection we need to add here is called articles. So that's defined in that database. And since we want to do keyword search, let's say before we go into keyword search, let's copy what these properties are like. So these will be, let's say, a list defined with this data set, like this. Every article in this database has a number of properties. What we're saying here is that the results for this search, we want you to return to us the title, the URL, and the text for each result. There are other properties in here, but we don't want the database to return them to us now. Now to do the keyword search part, Weaviate has us type ".with_bm25", and bm25 is this keyword search or lexical search algorithm commonly used, and it scores the documents in the archive versus the query based on a specific formula that looks at the count of the shared words between the query and each document and the way we need to do this is to say "query=query" we will pass to you and the query we need to add to this function so it is a parameter here as well. A couple more lines we need to pass to alleviate our ".with_where", so we can have a where clause here that is formatted in a specific way. So what we want to do here is limit this to only English results. And results slang will be something we also add to this definition. So let's say "en". By default, we'll filter by the English language and only look at the English language articles. So that's why we're adding it here as a default, but it's something we can change whenever we call the keyword search method. Now one more line we need to add is to say ".with_limit". So how many results do we want the search engine to retrieve to us? So, we say "num_results" and then we define that here as well, so "num_results". And let's set that by default to say 3. And with that, our query is complete and we just say do and then we close that request. And once we've completed this, we can now get the response and return the result. With this, that is our keyword search function. Now let's use this keyword search function and pass it one query. Say we say, what is the most viewed televised event? We pass the query to the function and then we print it. So let's see what happens when we run this. It goes and comes back and these are the search results that we have. It's a lot of text, we'll go through it, but we can see that it's a list of dictionaries. So let's define a function that prints it in maybe a better way. And this function can look like this "print_result". And with this, we can say, okay, now print it and let me see exactly what the results were. So the first result that we got back is this is the text. This is the passage or paragraph of the text. This is the title, and remember, we're trying to look for what is the most televised event. This does not really look like the correct result very much, but it contains a lot of the keywords. Now, we have another article here about the Super Bowl. This is a better result, so the Super Bowl could probably be a highly televised event. and then there's another result here that kind of mentions the World Cup but it's not exactly the World Cup result.  With each of these you see the URL of that article we can click on it and it will lead us to a Wikipedia page. You can try to edit this query so you can see what else is in this data set but this is a high-level example of the query connecting to the database and then seeing the results. A few things you can try here as well is you need to look at the properties. This is the list of properties that this data set was built using and so these are all columns that are stored within the database. So you can say you're gonna look at how many views a Wikipedia page received. You can use that to filter or sort. This is an estimated figure but then this is the language column that we use to filter, and you can use other values for language. The codes for the other languages look like this. So we have English, German, French, Spanish, Italian, Japanese, Arabic, Chinese, Korean, and Hindi, I believe. So just input one of these and pass it to the keyword search, and it will give you results in that language. Let's see how we can query the database with a different language. So let's say we copy this code. Notice that now we're printing the result here. Let's specify the language to a different language here. I'm going to be using, let's say, German. And we did German here because some words might be shared and we can see here some results. So this result for the most televised event is for Busta Rhymes, the musician. But you can see why it brought this as a result, right? Because the word event is here. And then the name of the album mentioned here is event. So the text here and the query that we have shared, they don't have to share all of the keywords but at least some of them. BM25 only needs one word to be shared for it to score that as somewhat relevant. And the more words the query and the document share, the more it's repeated in the document, the higher the score is. But we can see in general, while these results are returned, this is maybe not the best, most relevant answer to this question or document that is most relevant to this query. We'll see how language models help with this. So at the end of the first lesson, let's look back at search at a high level. The major components are the query, the search system, the search system has access to a document archive that it processed beforehand, and then in response to the query the system gives us a list of results ordered by the most relevant to the query. If we look a little bit more closely, we can think of search systems as having multiple stages. The first stage is often a retrieval or a search stage, and there's another stage after it called re-ranking.  Re-ranking is often needed because we want to involve or include additional signals rather than just text relevance. The first stage, the retrieval, commonly uses the BM25 algorithm to score the documents in the archive versus the query. The implementation of the first stage retrieval often contains this idea of an inverted index. Notice that this table is a little bit different than the table we showed you before of the documents. The inverted index is this kind of table that has kind of these two columns. One is the keyword, and then next to the keyword is the documents that this keyword is present in. This is done to optimize the speed of the search. When you enter a query into a search engine, you expect the results in a few milliseconds. This is how it's done. In practice, in addition to the document ID, the frequency of how many times this keyword appears is also added to this call. With this, you now have a good high-level overview of keyword search. Now, notice for this query, what color is the sky, when we look at the inverted index, the word color has the document 804, and the word sky also has the document 804. So 804 will be highly rated from the results that are retrieved in the first stage. From our understanding of keyword search, we can see some of the limitations. So, let's say we have this query, strong pain in the side of the head. If we search a document archive that has this other document that answers it exactly, but it uses different keywords, so it doesn't use them exactly, it says sharp temple headache, keyword search is not going to be able to retrieve this document. This is an area where language models can help, because they're not comparing keywords simply. They can look at the general meaning and they're able to retrieve a document like this for a query like this. Language models can improve both search stages and in the next lessons, we'll look at how to do that. We'll look at how language models can improve the retrieval or first stage using embeddings, which are the topic of the next lesson. And then we'll look at how re-ranking works and how it can improve the second stage. And at the end of this course, we'll look at how large language models can generate responses as informed by a search step that happens beforehand. So let's go to the next lesson and learn about embeddings.





## Embeddings


Welcome to Lesson 2. In this lesson you will learn embeddings. Embeddings are numerical representations of text that computers can more easily process. This makes them one of the most important components of large language models. So let's start with the embeddings lab. This code over here is going to help us load all the API keys we need. Now in the classroom this is all done for you, but if you'd like to do this yourself you would have to pip install some packages, for example, the Cohere one. Other packages you would have to install for the visualizations are umap-learn, Altair, and datasets for the Wikipedia dataset. I'm gonna comment this line because I don't need to run it in this classroom. So next, you'll import the Cohere library. The Cohere library is an extensive library of functions that use large language models and they can be called via API calls. In this lesson we're going to use the embed function but there are other functions like the generate function which you'll use later in the course. The next step is to create a Cohere client using the API key. So first let me tell you what an embedding is. Over here we have a grid with a horizontal and a vertical axis and coordinates, and we have a bunch of words located in this grid as you can see. Given the locations of these words, where would you put the word apple? As you can see in this embedding, similar words are grouped together. So in the top left you have sports, in the bottom left you have houses and buildings and castles, in the bottom right you have vehicles like bikes and cars, and in the top right you have fruits. So the apple would go among the fruits. Then the coordinates for Apple here are 5'5 because we are associating each word in the table in the right to two numbers, the horizontal and the vertical coordinate. This is an embedding. Now this embedding sends each word to two numbers like this. In general, embeddings would send words to a lot more numbers and we would have all the possible words. Embeddings that we use in practice could send a word to hundreds of different numbers or even thousands. Now let's import a package called pandas and we're going to call it "pd". Pandas is very good for dealing with tables of data. And the first table of data that we're going to use is a very small one. It has three words. The word joy, the word happiness, and the word potato which you can see over here. The next step is to create embeddings for these three words. We're going to call them three words emb and to create the embeddings we're going to call the cohere function embed. The embed function takes some inputs. The first one is the data set that we want to embed which is called three words for this table and we also have to specify the column which is called text. Next we specify which of the cohere models we want to use and finally we extract the embeddings from there. So now we have our three words embeddings. Now let's take a look at the vector associated with each one of the words. The one associated with word joy, we're going to call it "word_1". And the way we get it is by looking at "three_words_emb" and taking the first row. Now we're going to do the same thing with "word_2" and "word_3". Those are the vectors corresponding to the words happiness and potato. Just out of curiosity, let's look at the first 10 entries of the vector associated with the word joy. That's going to be "word_1" all the way up to 10. Now embeddings not only have to work for words, they can also work for longer pieces of text. Actually, it can be really long pieces of text. In this example here, we have embeddings for sentences. Now the sentences get sent to a vector or a list of numbers. And notice that that the first sentence is, hello, how are you? The last one is, hi, how's it going? And they don't have the same words, but they are very similar. And because they're very similar, the embedding sends them to numbers that are really close to each other. Now, let me show you an example of embeddings. First, we'll have to import Pandas as "pd". Pandas is a library that works for handling tables of data. And next, we're going to take a look at a small data set of sentences. This one has eight sentences, as you can see. They are in pairs. Each one is the answer to the previous one, for example, what color is the sky? The sky is blue. What is an apple? An apple is a fruit. Now we are going to plot this embedding and see which sentences are close or far from each other. In order to turn all these sentences into embeddings we are going to use the embed function from Cohere. So we're going to call this table m and we're going to call the endpoint co.embed. This function is going to give us all the embeddings and it takes some inputs. The first input is the table of sentences that we want to embed. So the table is called sentences and we have to specify the column, which is called a text. The next input is the name of the model we're going to use. Finally, we extract the embeddings from the output of this function. This function is going to give us a long list of numbers for each one of the sentences. Let's take a look at the first 10 entries of the embeddings of each of the first three sentences. And they are over here. Now how many numbers are associated to each one of the sentences? In this particular case it's 4096, but different embeddings have different lengths. Now we're going to visualize the embedding. For this we're going to call a function from utils called umapplot. Umapplot uses the packages umap and altair and and it produces this plot over here. Notice that this plot gives us eight points in pairs of two. And let's look what the pairs are. This one over here is the bear lives in the woods and the closest sentence is where does the bear live? Which makes sense because they are sentences that are quite similar. Let's look at these two over here. Here we have what is an apple and an apple is a fruit. Over here we have where is the World Cup? The World Cup is in Qatar. And over here we have what color is the sky and the sky is blue. So as you can see, the embedding put similar sentences in points that are close by and different sentences in points that are far away from each other. Notice something very particular. The closest sentence to any question is its particular answer. So we could technically use this to find the answer to a question by searching for the closest sentence. This is actually the basis for dense retrieval, which is what Jay is going to teach you in the very next video. Now feel free to add some more sentences or change these sentences completely, and then plot the embedding and see how it looks like. Now that you know how to embed a small data set of eight sentences, let's do it for a large data set. We're gonna work with a big data set of Wikipedia articles. Let's load the following data set. It has a bunch of articles with title, the text of the first paragraph, and the embedding of that first paragraph. And it has 2,000 articles. We're gonna import "NumPy" and a function that will help us visualize this plot very similar to the previous one. We're going to bring it down to two dimensions so that it's visible for us. The embedding is over here and notice that similar articles are in similar places. For example, over here you can find a lot of languages. In here, countries. Over here you're going to find a lot of kings and queens. And here you're going to find a lot of soccer players, and over here you're going to find artists. Feel free to explore this embedding and try to find where the topics are located. And that's it for embeddings. Now in the next lesson with Jay, you will be able to use embeddings to do dense retrieval. That is, to be able to search for the answer to a query in a big database.

### My Notes

Looks like (k-means, etc. etc.) clustering to me over vector space. All examples use a pre-trained language model, but my issue is how to train a LOCAL language model like that: design system requirement for me is: no external companies, no remote access APIs used, information does not leave the room ("anonymization" is, IMO, flawed (as shown by various complaints about copyright infringement by ChatGPT et al where people are able to produce content they wrote/created themselves and is chopped up but still retrievable from the foreign database)).
Anyway, looks to me like "embeddings" is teaching (learning) a neural network what words and phrases are related (hence same cluster; same neighbourhood); with this I can see how a search based on the "embeddings" i.e. the cluster identifiers ("coordinates" if you will) can produce more useful answers as it's not a FTS based on direct word/phrase ngrams any more: IFF you train the embeddings model to closely associate, say, "coprolites" and "fossilized dung", then a search for "coprolites" would also rank high any documents that do NOT use that word but talk about "fossilized excrement" instead. (Bonus points if your training included "excrement" + "dung" clustering as well, for then the "embeddings" vector would also hit the "excrement" word and rank would jump to the top accordingly.)

What I was pondering for some time now is how to get such a model trained "as you go" *locally*: explicitly creating a training set for the embeddings database is beyond the means of most, unless^📕, perhaps, one can go something like this: start with classical FTS (based on ngrams/skipgrams as I already intended), get your hits (documents), get the highest ranked keywords for that document and find matching documents for those as being "similar documents"; use that as a training set to help *cluster* similar documents. This would be unsupervised training for this to scale beyond a very low number of documents; the auto-trained clustering info can then be used to "find" additional documents with high rank (hopefully; this would mean we got our training + ranking right for this new approach) to be added to our original query results.


📕: this was the idea, but I didn't / don't know yet how to make this happen on average local hardware, as this stuff has all the smells of exploding storage and CPU costs; something to test and experiment with further.


Couple of things:
- a query will produce search results, but for response latency reasons we don't limit that to classic REST query-response cycles, but would rather need a query process that's running kinda "in the background", spitting out results as/when they are discovered. The second key element in this process is that they will NOT necessarily show up in ranking order, but each answer will come with a score and the client side will have to merge the new search result and re-sort the list on display. This also means the UI behaviour must be tolerant of search list updates updating the result list will users go through / select/mark / do other things with the list: "live updates while you work".
- as the unsupervised training would not result in very well defined clusters from the start, on its own, the clustering (model output / training result), should only gradually boost the ranking of related "embedding-based" matches. The "learn as you go" idea that sits at the back of my head is becoming clearer now, but still an unsolved problem: what I need to do is find a way to gradually improve the training by taking user feedback about the query responses: if I can get people to VERY QUICKLY (low user effort/cost!) mark/tag search results for each (or many/several) of their queries, I can "construct" a query-response training set from that to improve the embeddings being discovered: this becomes semi-supervised training in a round-about way as it becomes a "human sometimes in / influenced the loop".
- The lesson mentions using a 4K wide embeddings vector for the demo database; for a generic language model (such as the English one they're using), this size makes sense as it's a near-minimum size for learning meanings of words ("semantics"): average human active vocabulary sits at around 2K+ (higher ed) / 1-1.5K for "everyone", depending on which investigation/report you pick, so that's 2K wide for allowing a machine to learn the meaning of words, thesaurus/dictionary style. That leaves 2K for idioms, sayings and other (partial-)sentence like mappings/relationships. However, training such an animal would come at considerable cost when done on local John Doe hardware; I don't have the equipment for that either, so I'm looking at much reduced language models. Hm, "good enough" should be redefined as "lightly better" in my mind, perhaps...  :-)


