# LLM training video transcript + notes (DeepLearning.AI)

Large Language Models with Semantic Search


- [Introduction](https://learn.deeplearning.ai/large-language-models-semantic-search/lesson/1/introduction)
- [Keyword Search](https://learn.deeplearning.ai/large-language-models-semantic-search/lesson/2/keyword-search)[
- [Embeddings](https://learn.deeplearning.ai/large-language-models-semantic-search/lesson/3/embeddings)[
- [Dense Retrieval](https://learn.deeplearning.ai/large-language-models-semantic-search/lesson/4/dense-retrieval)[
- [ReRank](https://learn.deeplearning.ai/large-language-models-semantic-search/lesson/5/rerank)[
- [Generating Answers](https://learn.deeplearning.ai/large-language-models-semantic-search/lesson/6/generating-answers)[
- [Conclusion](https://learn.deeplearning.ai/large-language-models-semantic-search/lesson/7/conclusion)[
- [Course Feedback](https://learn.deeplearning.ai/large-language-models-semantic-search/course-feedback)[
- [Community](https://learn.deeplearning.ai/large-language-models-semantic-search/community)




## Introduction

Welcome to this short course, Large Language ModelsÂ with Semantic Search, built in partnership with Cohere.Â 

In this course, you'll learn how to incorporate large language models, orÂ LLMs, into information search in your own applications.Â For example, let's say you run a website with a lot of articles, pictureÂ Wikipedia for the sake of argument, orÂ a website with a lot of e-commerce products.Â 

Even before LLMs, it was common to have keyword search toÂ let people maybe search your site.Â But with LLMs, you can now do much more.Â First, you can let users ask questions that yourÂ system then searches your site or database to answer.Â Second, the LLM is also making the retrieve results moreÂ relevant to the meaning or the semantics of what the user isÂ asking about.Â 

I'd like to introduce the instructors for this course, JayÂ Allamar and Luis Serrano.Â Both Jay and Luis are experienced machine learning engineersÂ as well as educators.Â 
I've admired for a long time some highly referenced illustrationsÂ that Jay had created to explainÂ transformer networks.Â He's also co-authoring a book, Hands-On Large Language Models.Â Luis is the author of the book, Grokking Machine Learning,Â and he also taught with DeepLearning.ai.Â Math for Machine Learning.Â At Cohere, Jay and Luis, together with Neil Amir,Â have also been working on a site called.Â LLMU, and have a lot of experience teaching developers to use LLMs.Â So I was thrilled when they agreed toÂ teach semantic search with LLMs.Â 

Thanks, Andrew.Â What an incredible honor it is to beÂ teaching this course with you.Â Your machine learning course introduced me to machineÂ learning eight years ago, and continues to beÂ an inspiration to continue sharing what I learn.Â As you mentioned, Luis and I work at Cohere,Â so we get to advise others in the industry on how toÂ use and deploy large language models for variousÂ use cases.Â We are thrilled to be doing this courseÂ to give developers the tools they need toÂ build robust LLM powered apps.Â We're excited to share what we learned from ourÂ experience in the field.Â Thank you, Jay and Luis.Â Great to have you with us.Â 

This course consists of the following topics.Â 
First, it shows you how to use basic keyword search,Â which is also called lexical search, which powered a lot of searchÂ systems before large language models.Â It consists of finding the documents that hasÂ the highest amount of matching words with the query.Â 
Then you learn how to enhance this typeÂ of keyword search with a method called re-rank.Â As the name suggests, this then ranks the responsesÂ by relevance with the query.Â 
After this, you learn a more advanced method of search,Â which has vastly improved the results of keyword search,Â as it tries to use the actual meaningÂ or the actual semantic meaning of the textÂ with which to carry out the search.Â This method is called dense retrieval.Â 
This uses a very powerful tool in naturalÂ language processing called embeddings, which is a way toÂ associate a vector of numbers with every piece of text.Â Semantic search consists of finding the closest documentsÂ to the query in the space of embeddings.Â 
Similar to other models, search algorithms needÂ to be properly evaluated.Â You also learn effective ways to do this.Â 
Finally, since LLMs can be used to generate answers,Â you also learn how to plug in the search results into anÂ LLM and have it generate an answer based on them.Â Dense retrieval with embeddings vastly improvesÂ the question answering capabilities of an LLM as itÂ first searches for and retrieves theÂ relevant documents and it creates an answer from thisÂ retrieved information.Â Many people have contributed to this course.Â We're grateful for the hard work of Meor Amer, PatrickÂ Lewis, Nils Reimer, and Sebastian HofstÃ¤tter from Cohere, asÂ well as on the DeepLearning.ai side, Eddie ShyuÂ and Diala Ezzedine.Â In the first lesson, you will see how search was done before largeÂ language models.Â From there, we will show you how to improve search using LLMs,Â including tools such as embeddings and re-rank.Â That sounds great.Â And with that, let's dive in and go on to the next video.





## Keyword Search


Welcome to Lesson 1.Â In this lesson, you will learn how to use keyword searchÂ to answer questions using a database.Â Search is crucial to how we navigate the world.Â This includes search engines, but it also includes search withinÂ apps, so when you search Spotify or YouTube orÂ Google Maps for example.Â Companies and organizations also need to use keyword searchÂ or various methods of search to search their internal documents.Â Keyword search is the method most commonly usedÂ to build search systems.Â Let's look at how we can use a keyword search system and thenÂ look at how language models can improve thoseÂ systems.Â Now, in this code example, we'll connect to a database and doÂ some keyword search on it.Â The first cell installs "weaviate=client".Â You don't need to run this if you're running this from inside the classroom.Â Â But if you want to download this code and runÂ it on your own environment, client, you would want to install "weaviate=client".Â Â The first code cell we need to run loads the API keys we'llÂ need later in the lesson.Â And then now we can import Weaviate.Â This will allow us to connect to an online database.Â We'll talk about this database.Â Let's talk about what Weaviate is.Â Weaviate is an open source database.Â It has keyword search capabilities, but also vectorÂ search capabilities that rely on language models.Â The API key we'll be using here is public, thisÂ is part of a public demo, so the actual key is notÂ a secret and you can use it and we encourageÂ you to use it to access this demo database.Â Now that we've set the configurations for authentication, let'sÂ look at this code that connects the client toÂ the actual database.Â Now this database is a public database andÂ it contains 10 million records.Â These are records from Wikipedia.Â Each cell, each record, a row in this database,Â is a passage, is a paragraph from Wikipedia.Â These 10 million records are from 10 different languages.Â So one million of them are in EnglishÂ and the other nine million are in different languages.Â And we can choose and filter which languageÂ we want to query, and we'll see that later in this lab.Â After we run this line of code, we make sure that theÂ client is ready and connected.Â And if we get true, then that means thatÂ our local Weaviate client is able to connectÂ to the remote Weaviate database.Â And then now we're able to do a keyword search on this data set.Â Let's talk a little bit about keyword search beforeÂ we look at the code.Â So let's say you have the query, what color is the grass?Â And you're searching a very small archive that has these five texts, theseÂ five sentences.Â One says tomorrow is Saturday, one says the grass is green,Â the capital of Canada is Ottawa, the sky is blue,Â and a whale is a mammal.Â So this is a simple example of search.Â A high-level look at how keyword search works is toÂ compare how many words are in common betweenÂ the query and the documents.Â So if we compare how many words are in common between theÂ query and the first sentence, they only share the word is.Â And so that's one word they have in common.Â And we can see the counts of every sentence in this archive.Â We can see that the second sentence has the mostÂ words in common with the query, and so keyword search mightÂ retrieve that as the answer.Â So now that we're connected to the database, let's buildÂ out the function that will query it.Â Let's call it "keyword_search" and we'll be building this and going backÂ and forth.Â So the simplest thing we'll need to do here is to say "responseÂ = (" and then "client.query.get".Â Now everything we're doing here, this is Weaviate.Â So this is all defined by the Weaviate API.Â And it tells us the kind of data, I think the collectionÂ we need to add here is called articles.Â So that's defined in that database.Â And since we want to do keyword search, let'sÂ say before we go into keyword search, let's copyÂ what these properties are like.Â So these will be, let's say, a list defined with this data set, likeÂ this.Â Every article in this database has a number of properties.Â What we're saying here is that the results for this search, weÂ want you to return to us the title, the URL, andÂ the text for each result.Â There are other properties in here, but we don't want the database to returnÂ them to us now.Â Now to do the keyword search part, Weaviate has us type ".with_bm25", andÂ bm25 is this keyword search or lexical searchÂ algorithm commonly used,Â and it scores the documents in the archiveÂ versus the query based on a specific formulaÂ that looks at the count of the sharedÂ words between the query and each document andÂ the way we need to do this is to say "query=query" we will pass to youÂ and the query we need to add to this function so itÂ is a parameter here as well.Â A couple more lines we need to pass to alleviate our ".with_where",Â so we can have a where clause here that is formattedÂ in a specific way.Â So what we want to do here is limit this to only English results.Â And results slang will be something we alsoÂ add to this definition.Â So let's say "en".Â By default, we'll filter by the English language and only look atÂ the English language articles.Â So that's why we're adding it here as a default, but it'sÂ something we can change whenever we call the keyword searchÂ method.Â Now one more line we need to add is to say ".with_limit".Â So how many results do we want the search engine to retrieve to us?Â So, we say "num_results" and then we define that here as well, soÂ "num_results".Â And let's set that by default to say 3.Â And with that, our query is complete andÂ we just say do and then we close that request.Â And once we've completed this, we can now get the responseÂ and return the result.Â With this, that is our keyword search function.Â Now let's use this keyword search function and pass it one query.Â Say we say, what is the most viewed televised event?Â We pass the query to the function and then we print it.Â So let's see what happens when we run this.Â It goes and comes back and these areÂ the search results that we have.Â It's a lot of text, we'll go through it, but weÂ can see that it's a list of dictionaries.Â So let's define a function that prints itÂ in maybe a better way.Â And this function can look like this "print_result".Â And with this, we can say, okay, now print it and let meÂ see exactly what the results were.Â So the first result that we got back is this is the text.Â This is the passage or paragraph of the text.Â This is the title, and remember, we're trying to look for whatÂ is the most televised event.Â This does not really look like the correct result very much,Â but it contains a lot of the keywords.Â Now, we have another article here about the Super Bowl.Â This is a better result, so the Super Bowl could probablyÂ be a highly televised event. and then there'sÂ another result here that kind of mentions theÂ World Cup but it's not exactly the World Cup result.Â Â With each of these you see the URL of thatÂ article we can click on it and it will leadÂ us to a Wikipedia page.Â You can try to edit this query so you canÂ see what else is in this data set but this is a high-levelÂ example of the query connecting toÂ the database and then seeing the results.Â A few things you can try here as well isÂ you need to look at the properties.Â This is the list of properties that thisÂ data set was built using and so theseÂ are all columns that are stored within the database.Â So you can say you're gonna look at how manyÂ views a Wikipedia page received.Â You can use that to filter or sort.Â This is an estimated figure but then thisÂ is the language column that we use to filter,Â and you can use other values for language.Â The codes for the other languages look like this.Â So we have English, German, French, Spanish, Italian, Japanese, Arabic,Â Chinese, Korean, and Hindi, I believe.Â So just input one of these and pass it to the keyword search,Â and it will give you results in that language.Â Let's see how we can query the database withÂ a different language.Â So let's say we copy this code.Â Notice that now we're printing the result here.Â Let's specify the language to a different language here.Â I'm going to be using, let's say, German.Â And we did German here because some wordsÂ might be shared and we can see here some results.Â So this result for the most televised eventÂ is for Busta Rhymes, the musician.Â But you can see why it brought this as a result, right?Â Because the word event is here.Â And then the name of the album mentioned here is event.Â So the text here and the query that we have shared,Â they don't have to share all of the keywords butÂ at least some of them.Â BM25 only needs one word to be shared for itÂ to score that as somewhat relevant. And the more words theÂ query and the document share, the more it'sÂ repeated in the document, the higher the score is.Â But we can see in general, while these results are returned,Â this is maybe not the best,Â most relevant answer to this question or document thatÂ is most relevant to this query.Â We'll see how language models help with this.Â So at the end of the first lesson, let'sÂ look back at search at a high level.Â The major components are the query, the search system,Â the search system has access to a document archiveÂ that it processed beforehand, and then in responseÂ to the query the system gives us a list of results orderedÂ by the most relevant to the query.Â If we look a little bit more closely, we can think of searchÂ systems as having multiple stages.Â The first stage is often a retrieval or a search stage,Â and there's another stage after it called re-ranking.Â Â Re-ranking is often needed because we want to involve orÂ include additional signals rather than justÂ text relevance.Â The first stage, the retrieval, commonly uses the BM25 algorithm toÂ score the documents in the archive versus the query.Â The implementation of the first stageÂ retrieval often contains this idea of an inverted index.Â Notice that this table is a little bit different than the tableÂ we showed you before of the documents.Â The inverted index is this kind of table that hasÂ kind of these two columns.Â One is the keyword, and then next to theÂ keyword is the documents that this keyword is present in.Â This is done to optimize the speed of the search.Â When you enter a query into a search engine,Â you expect the results in a few milliseconds.Â This is how it's done.Â In practice, in addition to the document ID,Â the frequency of how many times this keyword appears is also addedÂ to this call.Â With this, you now have a good high-level overview of keyword search.Â Now, notice for this query, what color is the sky,Â when we look at the inverted index, the word color has the document 804,Â and the word sky also has the document 804.Â So 804 will be highly rated from theÂ results that are retrieved in the first stage.Â From our understanding of keyword search,Â we can see some of the limitations.Â So, let's say we have this query, strong pain in the side of the head.Â If we search a document archive that has this other document thatÂ answers it exactly, but it uses different keywords,Â so it doesn't use them exactly, it says sharp temple headache, keywordÂ search is not going to be able toÂ retrieve this document.Â This is an area where language models can help,Â because they're not comparing keywords simply.Â They can look at the general meaning and they'reÂ able to retrieve a document like this forÂ a query like this.Â Language models can improve both search stages andÂ in the next lessons, we'll look at how to do that.Â We'll look at how language models can improve the retrieval or first stageÂ using embeddings, which are the topic of the next lesson.Â And then we'll look at how re-ranking works and how it can improveÂ the second stage.Â And at the end of this course, we'll look at how large language modelsÂ can generate responses as informed by a search stepÂ that happens beforehand.Â So let's go to the next lesson and learn about embeddings.





## Embeddings


Welcome to Lesson 2.Â In this lesson you will learn embeddings.Â Embeddings are numerical representations of textÂ that computers can more easily process.Â This makes them one of the most importantÂ components of large language models.Â So let's start with the embeddings lab.Â This code over here is going to help us loadÂ all the API keys we need.Â Now in the classroom this is all done for you, but if you'dÂ like to do this yourself you would haveÂ to pip install some packages, for example, the Cohere one.Â Other packages you would have to install forÂ the visualizations are umap-learn, Altair, andÂ datasets for the Wikipedia dataset.Â I'm gonna comment this line because I don't needÂ to run it in this classroom.Â So next, you'll import the Cohere library.Â The Cohere library is an extensive library of functions that use largeÂ language models and they can be called via API calls.Â In this lesson we're going to use the embed function but there areÂ other functions like the generate functionÂ which you'll use later in the course.Â The next step is to create a Cohere client using the API key.Â So first let me tell you what an embedding is.Â Over here we have a grid with a horizontal andÂ a vertical axis and coordinates, and we have a bunchÂ of words located in this grid as you can see.Â Given the locations of these words, where would you put the word apple?Â As you can see in this embedding, similar words are grouped together.Â So in the top left you have sports, in the bottom left you haveÂ houses and buildings and castles,Â in the bottom right you have vehicles like bikes and cars,Â and in the top right you have fruits.Â So the apple would go among the fruits.Â Then the coordinates for Apple here are 5'5Â because we are associating each word in the table inÂ the right to two numbers, the horizontal and theÂ vertical coordinate.Â This is an embedding.Â Now this embedding sends each word to two numbers like this.Â In general, embeddings would send wordsÂ to a lot more numbers and we would have all the possible words.Â Embeddings that we use in practice could send a word to hundredsÂ of different numbers or even thousands.Â Now let's import a package called pandas and we're goingÂ to call it "pd".Â Pandas is very good for dealing with tables of data.Â And the first table of data that we're going to use is a very small one. ItÂ has three words.Â The word joy, the word happiness, and the word potato whichÂ you can see over here.Â The next step is to create embeddings for these threeÂ words.Â We're going to call them three words emb andÂ to create the embeddings we're going to call the cohereÂ function embed.Â The embed function takes some inputs.Â The first one is the data set that we want to embedÂ which is called three words for this tableÂ and we also have to specify the columnÂ which is called text.Â Next we specify which of the cohere models we want to useÂ and finally we extract the embeddings from there.Â So now we have our three words embeddings.Â Now let's take a look at the vector associated withÂ each one of the words.Â The one associated with word joy, we're going to call it "word_1".Â And the way we get it is by looking at "three_words_emb" and takingÂ the first row.Â Now we're going to do the same thing with "word_2"Â and "word_3".Â Those are the vectors corresponding toÂ the words happiness and potato.Â Just out of curiosity, let's look at the first 10Â entries of the vector associated with the word joy.Â That's going to be "word_1" all the way up to 10.Â Now embeddings not only have to work for words,Â they can also work for longer pieces of text.Â Actually, it can be really long pieces of text.Â In this example here, we have embeddings for sentences.Â Now the sentences get sent to a vector or a list of numbers.Â And notice that that the first sentence is, hello,Â how are you?Â The last one is, hi, how's it going?Â And they don't have the same words, but they are very similar.Â And because they're very similar, the embedding sends them toÂ numbers that are really close to each other.Â Now, let me show you an example of embeddings.Â First, we'll have to import Pandas as "pd".Â Pandas is a library that works for handling tables of data.Â And next, we're going to take a look atÂ a small data set of sentences.Â This one has eight sentences, as you can see.Â They are in pairs.Â Each one is the answer to the previous one,Â for example, what color is the sky?Â The sky is blue.Â What is an apple?Â An apple is a fruit.Â Now we are going to plot this embeddingÂ and see which sentences are close or far from each other.Â In order to turn all these sentences into embeddingsÂ we are going to use the embed function from Cohere.Â So we're going to call this table m and we're goingÂ to call the endpoint co.embed.Â This function is going to give us all the embeddingsÂ and it takes some inputs.Â The first input is the table of sentencesÂ that we want to embed.Â So the table is called sentences and weÂ have to specify the column, which is called a text.Â The next input is the name of the model we'reÂ going to use.Â Finally, we extract the embeddings fromÂ the output of this function.Â This function is going to give us a long list of numbersÂ for each one of the sentences.Â Let's take a look at the first 10 entries of the embeddings ofÂ each of the first three sentences.Â And they are over here.Â Now how many numbers are associated to eachÂ one of the sentences?Â In this particular case it's 4096, but different embeddings haveÂ different lengths.Â Now we're going to visualize the embedding.Â For this we're going to call a function from utils called umapplot.Â Umapplot uses the packages umap and altair and and it produces thisÂ plot over here.Â Notice that this plot gives us eight points in pairs of two.Â And let's look what the pairs are.Â This one over here is the bear lives in the woods andÂ the closest sentence is where does the bear live?Â Which makes sense because they are sentences that are quite similar.Â Let's look at these two over here.Â Here we have what is an apple and an apple is a fruit.Â Over here we have where is the World Cup?Â The World Cup is in Qatar.Â And over here we have what color is the skyÂ and the sky is blue.Â So as you can see, the embedding put similar sentences inÂ points that are close by and different sentences inÂ points that are far away from each other.Â Notice something very particular.Â The closest sentence to any question is its particularÂ answer.Â So we could technically use this to find the answer to aÂ question by searching for the closest sentence.Â This is actually the basis for dense retrieval,Â which is what Jay is going to teach you in the very next video.Â Now feel free to add some more sentencesÂ or change these sentences completely,Â and then plot the embedding and see how it looks like.Â Now that you know how to embed a small data set of eight sentences,Â let's do it for a large data set.Â We're gonna work with a big data set of Wikipedia articles.Â Let's load the following data set.Â It has a bunch of articles with title, the text of the first paragraph,Â and the embedding of that first paragraph.Â And it has 2,000 articles.Â We're gonna import "NumPy" and a function that willÂ help us visualize this plot very similar toÂ the previous one.Â We're going to bring it down to two dimensions so that it's visibleÂ for us.Â The embedding is over here and notice that similar articles are inÂ similar places.Â For example, over here you can find a lot of languages.Â In here, countries.Â Over here you're going to find a lot of kings and queens.Â And here you're going to find a lot of soccer players, andÂ over here you're going to find artists.Â Feel free to explore this embedding and try to find where theÂ topics are located.Â And that's it for embeddings.Â Now in the next lesson with Jay, you will be able toÂ use embeddings to do dense retrieval.Â That is, to be able to search for the answer to aÂ query in a big database.

### My Notes

Looks like (k-means, etc. etc.) clustering to me over vector space. All examples use a pre-trained language model, but my issue is how to train a LOCAL language model like that: design system requirement for me is: no external companies, no remote access APIs used, information does not leave the room ("anonymization" is, IMO, flawed (as shown by various complaints about copyright infringement by ChatGPT et al where people are able to produce content they wrote/created themselves and is chopped up but still retrievable from the foreign database)).
Anyway, looks to me like "embeddings" is teaching (learning) a neural network what words and phrases are related (hence same cluster; same neighbourhood); with this I can see how a search based on the "embeddings" i.e. the cluster identifiers ("coordinates" if you will) can produce more useful answers as it's not a FTS based on direct word/phrase ngrams any more: IFF you train the embeddings model to closely associate, say, "coprolites" and "fossilized dung", then a search for "coprolites" would also rank high any documents that do NOT use that word but talk about "fossilized excrement" instead. (Bonus points if your training included "excrement" + "dung" clustering as well, for then the "embeddings" vector would also hit the "excrement" word and rank would jump to the top accordingly.)

What I was pondering for some time now is how to get such a model trained "as you go" *locally*: explicitly creating a training set for the embeddings database is beyond the means of most, unless^ðŸ“•, perhaps, one can go something like this: start with classical FTS (based on ngrams/skipgrams as I already intended), get your hits (documents), get the highest ranked keywords for that document and find matching documents for those as being "similar documents"; use that as a training set to help *cluster* similar documents. This would be unsupervised training for this to scale beyond a very low number of documents; the auto-trained clustering info can then be used to "find" additional documents with high rank (hopefully; this would mean we got our training + ranking right for this new approach) to be added to our original query results.


ðŸ“•: this was the idea, but I didn't / don't know yet how to make this happen on average local hardware, as this stuff has all the smells of exploding storage and CPU costs; something to test and experiment with further.


Couple of things:
- a query will produce search results, but for response latency reasons we don't limit that to classic REST query-response cycles, but would rather need a query process that's running kinda "in the background", spitting out results as/when they are discovered. The second key element in this process is that they will NOT necessarily show up in ranking order, but each answer will come with a score and the client side will have to merge the new search result and re-sort the list on display. This also means the UI behaviour must be tolerant of search list updates updating the result list will users go through / select/mark / do other things with the list: "live updates while you work".
- as the unsupervised training would not result in very well defined clusters from the start, on its own, the clustering (model output / training result), should only gradually boost the ranking of related "embedding-based" matches. The "learn as you go" idea that sits at the back of my head is becoming clearer now, but still an unsolved problem: what I need to do is find a way to gradually improve the training by taking user feedback about the query responses: if I can get people to VERY QUICKLY (low user effort/cost!) mark/tag search results for each (or many/several) of their queries, I can "construct" a query-response training set from that to improve the embeddings being discovered: this becomes semi-supervised training in a round-about way as it becomes a "human sometimes in / influenced the loop".
- The lesson mentions using a 4K wide embeddings vector for the demo database; for a generic language model (such as the English one they're using), this size makes sense as it's a near-minimum size for learning meanings of words ("semantics"): average human active vocabulary sits at around 2K+ (higher ed) / 1-1.5K for "everyone", depending on which investigation/report you pick, so that's 2K wide for allowing a machine to learn the meaning of words, thesaurus/dictionary style. That leaves 2K for idioms, sayings and other (partial-)sentence like mappings/relationships. However, training such an animal would come at considerable cost when done on local John Doe hardware; I don't have the equipment for that either, so I'm looking at much reduced language models. Hm, "good enough" should be redefined as "lightly better" in my mind, perhaps...  :-)


