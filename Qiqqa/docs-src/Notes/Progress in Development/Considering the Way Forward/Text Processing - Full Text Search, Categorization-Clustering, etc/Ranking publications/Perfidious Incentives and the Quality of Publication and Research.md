# Perfidious Incentives and the Quality of Publication and Research

## Follow the money... though it might not always wiggle its *fine ass* too obviously

- Chinese doctors who don't get a promotion unless they can show they have published one or more papers: driving the demand for Paper Mills. Which offer fresh papers to publish, but also offer *Citation Stuffing* and other means to bump your *impact factor*. 
- We yak about those doctors, but how about the majority of university students every year? Or are those all undiscovered *Gustav Klimt*'s and *Marie Curie*s, brimming with creativity and innovative thought that's good for us all to get out there, *pronto*? Is it *sane* to require them to publish papers, which is arguably only most useful when you wish to climb the monkey tree within academia? What are we trying to measure, what are we trying to do it with and why do we need automated plagiat checks in the first place? Paper mills fill a gap and my primary worry is that gap, not the rodents that fill that hole in our living space.
- Tenure or no tenure, that's the question: the level of plausibly deniable fraud to get there and then *remain at the top* is, in my view, abhorrent.
- Supervisors, excelling at seagull management style, "encouraging" their pupils add their name to the author list... or you may find your life ever so slightly more interesting while you go towards your PhD/doctoral graduation, or filling supermarket shelves.
- Aggressive meat eaters were invented while waiting at the train station -- the publishing author got sacked after bowing to an aeon of international applause, but his entire encouraging environment remains comfortably settled right where they are. Monitor their environment and spot where those are off to next. The fish starts to rot at the very head.
- The phenomenon of [*predatory publishers*](https://www.google.com/search?q=aggessive+publisher) -- but don't you fret as the non-predatory ones are nice & sweat guys all. Nobody is bothered by the financial results (EBITA instead of ANOVA) reported at the yearly shareholders meetings; remunerations that are "market-conform" for the C-suite while reaching into the high heavens is laudable whatever you do (or did).
- Fraud isn't generally called "fraud" around here; that's a rather *plebian* word and we do not approve. Adapt and blend in: *learn the lingo*. It's "honest mistakes" all over the place. "Lost data", "inadequately kept/managed records", "less attentive than they should have been", "omissions resulting from undue time pressure", "differences in interpretation", etc. There's a whole *force majeure* vocabulary there that's daily use for Public Relations.
- *Plagiat*? When you're a student climbing the ranks, that's cause for being kicked out and blacklisted across the nation. However, when, on the other hand, you are near enough to the top and have therefore done your job and got the proper connections to be able to announce you discovered a better life and "moved on to another challenge to help humanity move forward", you are a dear survivor.
- Majorana particles? Easier to observe than my favorite mayonnaise in the overcrowded supermarket shelves. Cold fusion? Oops, sorry, some sparkling loose wires at the back, there, my mistake! WTF? How about a screwdriver and a soldering iron, to go with your complimentary electrician? *Nah*, that wouldn't be *fun*! And the list goes on, and *on*, *ad nauseam*...


## So what's our tool to do?

For each publication:
1. find out its DOI (as that helps with various APIs, I'm given to understand).
2. find out its current status at Retraction Watch and others. That's another couple of paywalls waiting to happen...
3. provide some means to "rank" the authors, publisher and supporting institution(s): are any or all of them *suspect*? Are they *tainted* somehow? When and how? And how does that relate to the paper we're checking out?

I *want* retracted papers to be available. Also in my own database.

However, what I need alongside is some sort of ranking/grading system which tells which of the collected papers are **more trustworthy**.

The rest of the world seems to focus on a metric for *most innovative* for a paper, but I consider that only a secundary measure now: if a paper is useful in other ways (e.g. better at explaining the concept / pointing at some requirements/assumptions and discussing their impact / being available to me, instead of hiding behind nauseous paywall number one-hundred-eleven) I'm fine with the given paper. What I *do* like to know while reading it is whether it's actual science, *empirical data* (which some consider less worthy, is my strong impression, alas), *fancy blather* or adorably veered *off the rails*: some times I like to read humorous publications, e.g. engineers using high-precision equipment to help them "discover" the earth is indisputably *flat*. Yeah, in your case I'd even argue it's positively *concave* as you were measuring across several hills and forgot to place that first reflector in a little gulley over there, instead of on top a lowly hill. ðŸ¥³ *Bliss*!

TL;DR? 
I should get a bit of an indicator / rank number telling me my machine is grading the **trustability** of the actual source of this paper 0-to-10, where 0 is "stinking all the way to high heaven: probably a Paper Mill running on chili-flavored crack and AI" and 10 is that unattainable level of *devine presage cogitation*. Ok, a 9 or 8 might be more reasonable for a *very* reliable source. 
ðŸ¤” Given what I've met to date, I'll start to worry about this rank number when it climbs above 7.

This is a machine-generated ranking that can sit next to my own opinion after reading the paper: what do I make of it in terms of Quality: 
- **readability** / well written?
- **informative**
- **assuming** a little or a lot (of background knowledge, etc.)
- **elucidating** ~ did it give *me* some new insights?  --  contrast this with general world-wide considered "*innovative*": some papers may be quite innovative but when they don't give me useful ideas to pursue or understand, they're less useful for me personally. Yes, this is an egocentric KPI.  *duh*

 The *trustability* ranking is a summary value signaling the quality of the author(s), their organizations and the publisher. ðŸ¤” on second thought, it might be good to also show the rankings of these 3 groups-of-people individually as it's perfectly well possible to have a reliable author publish via a *smelly* publisher -- or more probable: a *smelly* university. (Duke + cancer research, anyone? *interesting* environment they got there...)


- https://paolocrosetto.wordpress.com/2021/04/12/is-mdpi-a-predatory-publisher/
- https://scholarlykitchen.sspnet.org/2020/08/10/guest-post-mdpis-remarkable-growth/



## Post partum thoughts

Heck, I might include a plagiarism check alongside as well -- as someone pointed out to me, many electronic engineering papers out there are simply regurgitating (*plagiarizing?*) other papers and then an overlapping set has never gotten beyond the "successfully simulated in SPICE" stage, so you're looking at a lot of possible/probable noise. My pal [Donnie](https://en.wikipedia.org/wiki/There_are_unknown_unknowns) would've been elated: you can take it and run it any way you like.


And lastly, DO NOT FORGET there's plenty of other factions, plus "we are right because we are \[top dog\]" conservative crap going on from the established ranks: see the MDPI controversy. Yes, the buggers are aggressive, probably even *predatory*, but when you're part of a system where everyone is fighting to come out up on top and egos the size of Zeppelins reign supreme, while *almost all* areas of research suffer from a lack of peer reviewers' timely availability, I wonder if it might perchance be *rather inconvenient* to address that very issue. Because I see that one little datum as indicative of a *destructive* system, i.e. a system where particular costs are happily suppressed/ignored/you-name-it-what-you-like and specific functions are shoved under the rug in both respect and remuneration terms, at the general detriment of people. In other words: we're looking at a *rat race* setup, which requires excellence in a non-task-primary skill set to survive and rise to the top.

MDPI has found a great *gap* between supply and demand right there; unfortunately for everyone their board has smelled the green and those noses having gotten continually saturated with the *odour of dead presidents*, the trivial consequence is the glory of [Sin Number Three](https://en.wikipedia.org/wiki/Seven_deadly_sins), alas. Hence the *predatory* accusation is not without merit; on the contrary. 
But the alternative was -- and still is -- taking one of those *non-predatory* publishers to bed, one who won't demand you cough up somewhere around USD 1500 for your latest brainchild to get out into the world -- and within fathomable *youtube time* at that -- helping you getting a leg up in the Realm Of Tenure, instead opting for strangulating libraries and all readers the world around, demanding insane fees to obtain a copy of your humble produce through their *catalog subscriptions*. Of course, *the latter approach* isn't predatory as it isn't *you* who's got to pay, not just yet anyway, so why bother! Elsevier et al are *nice*. [sci-pub](https://sci-hub.ru/) got so big for a reason or two, but that's worth another rant another day.

*Ciao* and before you go off: grab your blood pressure meds, for we do care. ðŸ˜˜

